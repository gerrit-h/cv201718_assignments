{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Semantic Segmentation\n",
    "Heute werden wir einfache Netzwerkarchitekturen für \"Semantic Segmentation\" testen. Ziel ist es dieses Paper in den Grundzügen zu implementieren: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf. Bitte lesen!\n",
    "\n",
    "## Daten\n",
    "\n",
    "Es gibt einige gute Datensätze, die ihr (bei gegebener Hardware) herunterladen und benutzen könnt. Für diejenigen, die auf CPUs rechnen gilt immer der Tip: Bilder downsamplen!\n",
    "\n",
    "Sucht Euch einen Satensatz aus: \n",
    "\n",
    "KITTI: http://www.cvlibs.net/download.php?file=data_semantics.zip (~300 MB, 200 Bilder)\n",
    "\n",
    "DUS: http://www.6d-vision.com/scene-labeling (~3 GB, 500 Bilder)\n",
    "\n",
    "MIT. http://sceneparsing.csail.mit.edu/ (~1 GB, links siehe auf Seite)\n",
    "\n",
    "## Exc. 7.1 Fully convolutional network, no downsampling\n",
    "Implementiere die in der Vorlesung besprochene Netzwerkarchitektur von aufeinanderfolgenden CONV-Schichten (stride=1, mit zero-padding), um eine Ausgabeschicht zu bekommen, die die Eingabegröße aufweist. Tip: die letzte CONV-Schicht sollte eine Tiefe haben, die zur Zahl der Klassen korrespondiert. Benutze den L2-Loss zum Labelbild (Achtung: ihr müsst dafür entweder das Labelbild oder den Ausgabetensor umformulieren).\n",
    "\n",
    "Trainiere das Netzwerk auf den von Dir gewählten Datensatz und zeige den Verlauf des Losses, und einige zufällig gewählte Beispielbilder mit ihren vorhergesagten Segmentierungen an. (**RESULT**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_generator = ImageDataGenerator()\n",
    "label_generator = ImageDataGenerator()\n",
    "\n",
    "# Beispiel für den KITTI-Datensatz. Ich habe die 200 training samples in 180 train- und 20 testbilder\n",
    "# geteilt (macht 180 samples inkl. labels)\n",
    "# um uns das Leben leichter zu machen, Bilder heruntersamplen\n",
    "img_size = (38, 125)\n",
    "\n",
    "\n",
    "# Bild- und Label-Generator\n",
    "Q1 = image_generator.flow_from_directory( 'data_semantics/training/image_2',\n",
    "                                        class_mode=None,\n",
    "                                        batch_size=1,\n",
    "                                        target_size=img_size, seed=1)\n",
    "\n",
    "Q2 = label_generator.flow_from_directory( 'data_semantics/training/semantic_rgb',\n",
    "                                        class_mode=None,\n",
    "                                        batch_size=1,\n",
    "                                        target_size=img_size, seed=1)\n",
    "\n",
    "# ... kombinieren\n",
    "train_generator = zip(Q1, Q2)\n",
    "\n",
    "# mach der Definition des Modells:\n",
    "# model.fit_generator( train_generator, steps_per_epoch = 1000, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(4, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01), input_shape=img_size + (3,)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(4, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(3, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_144 (Conv2D)          (None, 38, 125, 4)        112       \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 38, 125, 4)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_145 (Conv2D)          (None, 38, 125, 8)        296       \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 38, 125, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_146 (Conv2D)          (None, 38, 125, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 38, 125, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_147 (Conv2D)          (None, 38, 125, 16)       1168      \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 38, 125, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_148 (Conv2D)          (None, 38, 125, 16)       2320      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 38, 125, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 38, 125, 8)        1160      \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 38, 125, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 38, 125, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 38, 125, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 38, 125, 4)        292       \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 38, 125, 4)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_152 (Conv2D)          (None, 38, 125, 3)        111       \n",
      "=================================================================\n",
      "Total params: 6,627\n",
      "Trainable params: 6,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4258\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 13s 50ms/step - loss: nan - acc: 0.4185\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 12s 48ms/step - loss: nan - acc: 0.4216\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4200\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4138\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4316\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4116\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: nan - acc: 0.4225\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4204\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4155\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4237\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 13s 52ms/step - loss: nan - acc: 0.4130\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4336\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 12s 47ms/step - loss: nan - acc: 0.4157\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4187\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4219\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4153\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4174\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4252\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4140\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 12s 48ms/step - loss: nan - acc: 0.4271\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 13s 51ms/step - loss: nan - acc: 0.4265\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 12s 50ms/step - loss: nan - acc: 0.4134\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 13s 52ms/step - loss: nan - acc: 0.4217\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 13s 54ms/step - loss: nan - acc: 0.4251\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4109\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4296\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4169\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4165\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4218\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: nan - acc: 0.4211\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: nan - acc: 0.4197\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 12s 50ms/step - loss: nan - acc: 0.4234\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 12s 47ms/step - loss: nan - acc: 0.4152\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4258\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4188\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4215\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4139\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4226\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 11s 46ms/step - loss: nan - acc: 0.4180\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4278\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4146\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4227\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 12s 47ms/step - loss: nan - acc: 0.4205\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 12s 47ms/step - loss: nan - acc: 0.4124\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 12s 48ms/step - loss: nan - acc: 0.4269\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 12s 46ms/step - loss: nan - acc: 0.4214\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4143\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 11s 45ms/step - loss: nan - acc: 0.4204\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: nan - acc: 0.4223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1827b15a20>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch = 250, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uns ist nicht ganz klar, warum kein loss angezeigt wird. Eventuell liegt es an der Ordnerstruktur der Daten, die wir angepasst haben, damit die Generator überhaupt die Bilder einlesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exc. 7.2 FCN mit Bottleneck\n",
    "\n",
    "Implementiere jetzt die Variante mit schrittweisem Down- und Upsampling, wie in der Vorlesung besprochen. Nutze dafür ein bestehendes Netzwerk (z.B. VGG16, https://keras.io/applications/#vgg16), entferne die FC-Schichten am Ende, und füge dann die Upsampling-Schichten hinzu. Wie in der vorigen Vorlesung zu Transfer Learning beschrieben, kannst Du jetzt nur den zweiten Teil trainieren und die Gewichte des ersten Teils \"einfrieren\".\n",
    "\n",
    "Stelle wie oben den Verlauf des Losses dar und wähle einige Beispielbilder aus dem Testset und zeige sie mit ihrer vorhergesagten Segmentierung an. (**BONUS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
